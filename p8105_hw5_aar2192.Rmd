---
title: "p8105_hw5_aar2192"
author: "Amadeia Rector"
date: "11/6/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
library(tidyverse)
library(purrr)
library(ggplot2)
theme_set(theme_bw() + theme(legend.position = "bottom"))
```

# Problem 1

## Part A

- Start with a dataframe containing all file names; the list.files function will help

```{r}
longitudinal_df =
    tibble(file_name = list.files(path = "./data")) 

```
This created a data frame with just the names of the csv files.

## Part B

- Iterate over file names and read in data for each subject using purrr::map and saving the result as a new variable in the dataframe

```{r}
  longitudinal_df =
    tibble(participant = list.files(path = "./data"),
           csv_file = str_c(path = "./data/", participant)) %>% 
    mutate(data = map(csv_file, read_csv)) %>% 
    unnest 
```
Using map, I read a string with the complete file names using the "read_csv" function.

## Part C

- Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary

```{r}
longitudinal_df =
    tibble(participant = list.files(path = "./data"),
           csv_file = str_c(path = "./data/", participant)) %>% 
    mutate(data = map(csv_file, read_csv)) %>% 
    unnest %>%
    separate(participant, into = c("participant_id", "trash"), sep = ".c") %>% 
  mutate(study_arm = participant_id) %>%
  mutate(study_arm = str_replace(study_arm, "con", "control")) %>% 
  mutate(study_arm = str_replace(study_arm, "exp", "experiment")) %>% 
  separate(study_arm, into = c("study_arm", "trash1"), sep = "_") %>% 
  select(participant_id, study_arm, everything(), -starts_with("trash"), -csv_file) %>% 
  janitor::clean_names() %>% 
  gather(key = week, value = observation, week_1:week_8) %>% 
  separate(week, into = c("trash", "week"), sep = "_") %>% 
  select(-trash) 
```
Building on Part B, I tidied my newly created data set by renaming some variables, removing unnecessary variables, creating a variable based on study arm, and gathering the week variables into one column. 

## Part D

- Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

```{r}
longitudinal_df %>% 
  ggplot(aes(x = week, y = observation, group = participant_id, color = study_arm)) +
  geom_path() + 
  labs(title = "Observations over Time per Subject",
       x = "Time (weeks)",
       y = "Observations") + 
  viridis::scale_color_viridis(
    name = "Study arm", 
    discrete = TRUE) +
  theme(plot.title = element_text(hjust = 0.5))
```
 
### Plot 1 description:

Given the spaghetti plot above, it appears that subjects in the **"experiment"** arm have _higher_ observations overall compared to the **"control"** arm.  

While there is some overlap/crossing of lines between the "control" and "experiment" subjects with regard to observations, it seems that the highest observation among subjects in the "control" arm is at around 4.5 compared to above 7.5 among subjects in the "experiment" arm. The lowest observation is slightly above -2.5 among subjects in the "control" arm compared to just above -1 among subjects in the "experiment" arm.

# Problem 2

## Part A

- Describe the raw data. 

```{r}
wp_murders_df = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")

str(wp_murders_df)

table(wp_murders_df$victim_sex)
unique(wp_murders_df$city)
unique(wp_murders_df$state)
summary(as.numeric(wp_murders_df$victim_age), na.rm = TRUE)
sort(table(wp_murders_df$city), decreasing = TRUE)
sort(table(wp_murders_df$victim_race), decreasing = TRUE)
```
There are **52179 observations** with **12 variables** in this dataset, which are "uid", "reported_date", "victim_last", "victim_first", "victim_race", victim_age", "victim_sex", "city", "state", "lat", "lon", and "disposition".  
The data include the _location_ of the killing (which is defined by 4 variables "city", "state", "lat", and "lon"), whether an _arrest_ was made (defined by the variable "disposition"), the _reported date_ of the homicide ("reported_date") and information about each victim, such as _name_, _race_, _sex_, _age_, and their _homicide ID_ (captured by the variables "victim_last", "victim_first", "victim_race", victim_sex" "victim_age", and "uid").  

Of the 52,179 homicides, **7209** were female, **40739** were male, and 4,231 were unknown. The data came from **50 large cities** from **28 states**. The homicides span from _2007_ to _2017_ for most cities. Ages of homicides ranged from **0** (presumed to be less than a year old) to **102**, with a median of **28 years of age**. There were 2999 homicides without age reported (NA's). The 5 cities with the most homicides were _Chicago_ (5535), _Philadelphia_ (3037), _Houston_ (2942), _Baltimore_ (2827), and _Detroit_ (2519). Most of the homicide victims were of Black race (33361).

## Part B

- Create a city_state variable (e.g. “Baltimore, MD”) 

```{r}
wp_murders_df =
  wp_murders_df %>% 
  mutate(state = replace(state, city=="Tulsa", "OK")) %>% 
  mutate(city_state = paste(city, state, sep = "_")) %>% 
  select(uid:lon, city, state, city_state, disposition)
  

summary(unique(wp_murders_df$city_state))
sort(table(wp_murders_df$city_state), decreasing = TRUE)
```
A new variable, "city_state", has been created using mutate and paste. When originally creating the new variable, it was noticed that there were 51 unique values for "city_state" as opposed to 50 (this was discovered by looking at the commands "unique(wp_murders_df$city_state)" and "sort(table(wp_murders_df$city_state), decreasing = TRUE)". It seems that for one observation, the city Tulsa was accidentally cross referenced to AL instead of OK. This was corrected.

## Part C

- Summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r}
wp_murders_df %>% 
  group_by(city, disposition) %>% 
  summarize(n = n()) %>% 
  spread(key = disposition, value = n) %>% 
  janitor::clean_names() %>% 
  mutate(n_unsolved = sum(closed_without_arrest, open_no_arrest, na.rm=TRUE)) %>% 
  mutate(n_homicide = sum(closed_without_arrest, open_no_arrest, closed_by_arrest, na.rm=TRUE)) %>% 
  select(-closed_by_arrest, -closed_without_arrest, -open_no_arrest) %>% 
  arrange(-n_unsolved)

```
As seen by the grouped and arranged dataframe above, the cities with the highest number of unsolved homicides are Chicago (4073), Baltimore (1825), Houston (1493), Detroit (1482), and Philadelphia (1360).

## Part D

- For the city of Baltimore, MD, use the prop.test function to estimate the **proportion of homicides** that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}
## currently not sure what prop.test does!!!
wp_murders_city =
  wp_murders_df %>% 
  group_by(city, disposition) %>% 
  summarize(n = n()) %>% 
  spread(key = disposition, value = n) %>% 
  janitor::clean_names() %>% 
  mutate(n_unsolved = sum(closed_without_arrest, open_no_arrest, na.rm=TRUE)) %>% 
  mutate(n_homicide = sum(closed_without_arrest, open_no_arrest, closed_by_arrest, na.rm=TRUE)) %>% 
  select(-closed_by_arrest, -closed_without_arrest, -open_no_arrest) %>%
  mutate(n_homicide = as.numeric(n_homicide), n_unsolved = as.numeric(n_unsolved)) %>% 
  ungroup() 

baltimore = wp_murders_city %>% 
  filter(city == "Baltimore")

prop_res_balt = prop.test(baltimore$n_unsolved, baltimore$n_homicide)

broom::tidy(prop_res_balt) %>% 
  select(estimate, conf.low, conf.high)  
```

In this chunk, I created a dataframe from the grouping I did earlier and then created a new dataframe with Baltimore as the only city. I applied prop.test to this newly created dataframe (but I didn't do this using pipes). I saved the output of the prop.test results and applied broom::tidy so as to just create a dataframe containing the estimate and the upper and lower confidence intervals.

## Part E

- Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and  unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

```{r}
prop_new = function(x,n){
  output = prop.test(x,n)
  broom::tidy(output)
}

wp_murders_city %>% 
  mutate(prop = map2(n_unsolved,n_homicide, prop_new) ) %>% 
  unnest() %>% 
  select(city, estimate, conf.low, conf.high)

```
In the code chunk above, I created a function that included the prop.test and broom::tidy function, then I applied this to the full dataframe by applying map2.

## Part F

- Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r}
wp_murders_city %>% 
  mutate(prop = map2(n_unsolved,n_homicide, prop_new) ) %>% 
  unnest() %>% 
  select(city, estimate, conf.low, conf.high) %>% 
  mutate(city = fct_reorder(city, estimate)) %>% 
  ggplot() + geom_errorbar(mapping = aes(x = city, ymin = conf.low, ymax = conf.high)) +
  geom_point(mapping = aes(x = city, y = estimate, color = city )) +
  labs( title = "Proportion of Unsolved Homicides by City",
        y = "Unsolved Homicides (%)",
        x = "City") +
  theme(legend.position = 'none', axis.text.x = element_text(angle = 90, vjust =0.5, hjust = 1), plot.title = element_text(hjust = 0.5)) +
  scale_x_discrete(expand = expand_scale(add = 0.4))
  
```

### Plot 2 description:  

This graph is organized in ascending order from cities with a low proportion of unsolved homicides to a high proportion. From what we can see, Chicago has the highest proportion while Ricmond has the lowest among the 50 cities. We can also see that Tampa and Savanah have wider confidence intervals for the proportion of unsolved homicides, while Chicago has the smallest confidence interval.

